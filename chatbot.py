# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KI12cuNan0OIu-XIEXeUZWGPf3I57W8O

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cindyweng18/ccnychatbot/blob/main/chatbot.ipynb)

# CCNY Chatbot
### Team Members: Cindy Weng Zhu, David Jimenez, Sheriff Sanni, Nahim Imtiaz, Sajid Mahmud

Note: Please make sure to have Runtime Type as GPU

## Installs and Imports
"""

# Download questions cvs (questions to labels/intents)
!gdown "https://drive.google.com/uc?id=1uu6-ExY9OWkGmO9LuH1YP1Y1v7zOIx6t&export=download"

# Download answers.json (matches intents to potential answers)
!gdown "https://drive.google.com/uc?id=1NLQ6yJlsdVctQqaV_AYsq71hEOdcw_RB&export=download"

# Install transformers
!pip install transformers

# Install torchinfo to get model summary/info
!pip install torchinfo

import numpy as np
import pandas as pd
import re
import torch
import random
import torch.nn as nn
import transformers
import matplotlib.pyplot as plt
import seaborn as sns
import torchinfo
import json
import nltk
import spacy
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer
from textblob import TextBlob

device = torch.device("cuda")

"""## Preprocessing"""

df = pd.read_csv("/content/test_chatbot.csv")

df.head()

df.describe()

# Converting the labels into encodings
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Label'] = le.fit_transform(df['Label'])
# check class distribution
df['Label'].value_counts(normalize = True)

#converting Text column to lower case and remove punctuations
# df['Text'] = df['Text'].str.lower().apply(lambda x: re.sub(r'[.,"\'-?:!;]', '', x))

# stop_words = stopwords.words('english')

# df['text_without_stopwords'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
# df.head()
# stop_words

# df['Text']

# lemmatizer = WordNetLemmatizer()
# tokenizer = WhitespaceTokenizer()
# nlp = spacy.load('en', disable=['parser', 'ner'])

# df.head(10)

# df['lem_text'] = df['text_without_stopwords'].apply(lambda row: " ".join([w.lemma_ for w in nlp(row)]))
# df.tail(20)

# Fix misspelled words
# df['corrected_words'] = df['lem_text'].apply(lambda x: str(TextBlob(x).correct()))
# df.tail(20)

train_text, train_labels = df['Text'], df['Label']

from transformers import DistilBertTokenizer, DistilBertModel
# Load the DistilBert tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
# Import the DistilBert pretrained model
bert = DistilBertModel.from_pretrained("distilbert-base-uncased")

# get length of all the messages in the train set
seq_len = [len(i.split()) for i in train_text]
pd.Series(seq_len).hist(bins = 10)
max_seq_len = 10

# get length of all the messages in the train set
# seq_len = [len(i.split()) for i in df['text_without_stopwords']]
# pd.Series(seq_len).hist(bins = 10)
# max_seq_len = 10

# get length of all the messages in the train set
# seq_len = [len(i.split()) for i in df['lem_text']]
# pd.Series(seq_len).hist(bins = 10)
# max_seq_len = 10

# tokenize and encode sequences in the training set
tokens_train = tokenizer(
    train_text.tolist(),
    max_length = max_seq_len,
    padding=True,
    truncation=True,
    return_token_type_ids=False
)

import nltk
from nltk import tokenize

tokenSpace = tokenize.WhitespaceTokenizer()
def counter(text, columnText, quantity):
    allWords = ' '.join([text for text in text[columnText].astype('str')])
    tokenPhrase = tokenSpace.tokenize(allWords)
    frequency = nltk.FreqDist(tokenPhrase) 
    dfFrequency = pd.DataFrame({"Word": list(frequency.keys()), "Frequency": list(frequency.values())}) 
    
    dfFrequency = dfFrequency.nlargest(columns = "Frequency", n = quantity)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data = dfFrequency, x = "Word", y = "Frequency", palette="deep")
    ax.set(ylabel = "Count")
    plt.xticks(rotation='horizontal')
    plt.show()

counter(df, 'Text', 15)

# counter(df, 'text_without_stopwords', 15)

# counter(df, 'lem_text', 15)

# for train set
train_seq = torch.tensor(tokens_train['input_ids'])
train_mask = torch.tensor(tokens_train['attention_mask'])
train_y = torch.tensor(train_labels.tolist())

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
#define a batch size
batch_size = 16
# wrap tensors
train_data = TensorDataset(train_seq, train_mask, train_y)
# sampler for sampling the data during training
train_sampler = RandomSampler(train_data)
# DataLoader for train set
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

"""## Model"""

class BERT_Model(nn.Module):
    def __init__(self, bert):      
       super(BERT_Model, self).__init__()
       self.bert = bert 
      
       # dropout layer
       self.dropout = nn.Dropout(0.2)
      
       # relu activation function
       self.relu =  nn.ReLU()
       # dense layer
       self.fc1 = nn.Linear(768,512)
       self.fc2 = nn.Linear(512,256)
       self.fc3 = nn.Linear(256,121)
       #softmax activation function
       self.softmax = nn.LogSoftmax(dim=1)
       #define the forward pass
    def forward(self, sent_id, mask):
      #pass the inputs to the model  
      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]
      
      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)
      
      x = self.fc2(x)
      x = self.relu(x)
      x = self.dropout(x)
      # output layer
      x = self.fc3(x)
   
      # apply softmax activation
      x = self.softmax(x)
      return x

for param in bert.parameters():
      param.requires_grad = False
model = BERT_Model(bert)
# push the model to GPU
model = model.to(device)
# from torchinfo import summary
# summary(model)

from transformers import AdamW
# define the optimizer
optimizer = AdamW(model.parameters(), lr = 1e-3)

from sklearn.utils.class_weight import compute_class_weight
#compute the class weights
class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)
print(class_wts)

# convert class weights to tensor
weights= torch.tensor(class_wts,dtype=torch.float)
weights = weights.to(device)
# loss function
cross_entropy = nn.NLLLoss(weight=weights)

# empty lists to store training and validation loss of each epoch
train_losses=[]
# number of training epochs
epochs = 200
# We can also use learning rate scheduler to achieve better results
# lr_sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)

def train():
  
    model.train()
    total_loss = 0
  
  # empty list to save model predictions
    total_preds=[]
  
  # iterate over batches
    for step,batch in enumerate(train_dataloader):
    
    # progress update after every 50 batches.
        if step % 50 == 0 and not step == 0:
            print('  Batch {:>121,}  of  {:>121,}.'.format(step,    len(train_dataloader)))
        # push the batch to gpu
        batch = [r.to(device) for r in batch] 
        sent_id, mask, labels = batch
  
        # get model predictions for the current batch
        preds = model(sent_id, mask)
   
        # compute the loss between actual and predicted values
        loss = cross_entropy(preds, labels)
        # add on to the total loss
        total_loss = total_loss + loss.item()
        # backward pass to calculate the gradients
        loss.backward()
        # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        # update parameters
        optimizer.step()
        # clear calculated gradients
        optimizer.zero_grad()

        # We are not using learning rate scheduler as of now
        # lr_sch.step()
        # model predictions are stored on GPU. So, push it to CPU
        preds=preds.detach().cpu().numpy()
        # append the model predictions
        total_preds.append(preds)
    # compute the training loss of the epoch
    avg_loss = total_loss / len(train_dataloader)

    # predictions are in the form of (no. of batches, size of batch, no. of classes).
    # reshape the predictions in form of (number of samples, no. of classes)
    total_preds  = np.concatenate(total_preds, axis=0)
    #returns the loss and predictions
    return avg_loss, total_preds

for epoch in range(epochs):
     
  print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

  #train model
  train_loss, _ = train()

  # append training and validation loss
  train_losses.append(train_loss)
  # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.
  torch.backends.cudnn.deterministic = True
  torch.backends.cudnn.benchmark = False
  print(f'\nTraining Loss: {train_loss:.3f}')

plt.plot(train_losses)
plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

"""## Testing and Prediction"""

data = json.load(open("/content/answers.json"))

def get_prediction(str):
  str = re.sub(r'[^a-zA-Z ]+', '', str)
  test_text = [str]
  model.eval()

  tokens_test_data = tokenizer(
  test_text,
  max_length = max_seq_len,
  padding=True,
  truncation=True,
  return_token_type_ids=False
  )
  test_seq = torch.tensor(tokens_test_data['input_ids'])
  test_mask = torch.tensor(tokens_test_data['attention_mask'])
 
  result = None
  with torch.no_grad():
    preds = model(test_seq.to(device), test_mask.to(device))
    preds = preds.detach().cpu().numpy()
    result = preds[0]
    preds_index = np.argmax(preds, axis = 1)

  print("Intent Identified: ", le.inverse_transform(preds_index)[0])
  return le.inverse_transform(preds_index)[0]

def get_response(message): 
  intent = get_prediction(message)
  result = ""
  for i in data['intents']: 
    if i["tag"] == intent:
      result = random.choice(i["responses"])
      break
  print(f"Response : {result}")
  return "Intent: "+ intent + '\n' + "Response: " + result

# user_text = input("Type something: ")
# get_response(user_text)

get_response("i want to apply to city college")

get_prediction("How difficult is the workload at the school of medicine?")

get_response("Who is the director of the honors program?")
